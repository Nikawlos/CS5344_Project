'''
get all the scores

[['comment', 'hi1vs5n', '2qwzb', 'pregnant', 'FALSE', '1635206397', 'https://old.reddit.com/r/pregnant/comments/qfsajl/done_with_toxic_people_posting_false_information/hi1vs5n/', '``', 'I', '’', 'm', 'just', 'waiting', 'until', 'after', 'pregnancy', 'to', 'get', 'vaccinated', '.', 'I', 'have', 'a', 'special', 'needs', 'daughter', 'due', 'to', 'having', 'fever', 'during', 'pregnancy', 'and', 'I', '’', 'm', 'not', 'willing', 'to', 'risk', 'the', 'vaccine', 'fever', 'right', 'now', '.', 'I', 'have', 'strong', 'antibodies', 'from', 'Covid', 'that', 'I', 'had', 'before', 'I', 'got', 'pregnant', '.', '(', 'I', 'keep', 'taking', 'blood', 'tests', 'to', 'check', ')', 'There', '’', 's', 'a', 'difference', 'between', 'anti', 'vaccine', 'and', 'deciding', 'to', 'wait', 'based', 'on', 'different', 'factors', "''", '0.672', '1', 0.0, 0.02531645569620253, 0.012658227848101266, 0.02531645569620253, 0.02531645569620253, 0.0379746835443038, 0.0379746835443038, 0.0, 0.0, 0.0], ['comment', 'hi1vsag', '2riyy', 'nova', 'FALSE', '1635206399', 'https://old.reddit.com/r/nova/comments/qfs53d/2h_waittime_for_a_scheduled_booster_shot_at/hi1vsag/', 'When', 'you', 'scheduled', 'your', 'booster', 'with', 'CVS', 'does', 'it', 'just', 'give', 'you', 'the', 'option', 'of', 'Vaccines', ':', 'COVID-19', '(', 'Vaccine', 'brand', ')', 'or', 'does', 'it', 'specifically', 'say', 'booster', 'idiotic', '0', '2', 0.034482758620689655, 0.0, 0.034482758620689655, 0.0, 0.0, 0.034482758620689655, 0.034482758620689655, 0.0, 0.0, 0.0], ['comment', 'hi1vs7i', '2qhov', 'vancouver', 'FALSE', '1635206397', 'https://old.reddit.com/r/vancouver/comments/qft2x3/bc_takes_note_as_new_zealand_moves_to_ban/hi1vs7i/', '``', 'Did', "n't", 'stop', 'prices', 'there', 'though', '.', 'New', 'Zealand', 'and', 'Canada', 'grew', 'at', 'about', 'the', 'same', 'rate', 'through', 'COVID', '.', 'I', 'agree', 'that', 'non-resident', 'ownership', 'should', 'be', 'stopped', 'but', 'it', "'s", 'also', 'probably', 'not', 'going', 'to', 'change', 'anything', '.', "''", '0.1887', '32', 0.0, 0.0, 0.0, 0.024390243902439025, 0.0, 0.0, 0.04878048780487805, 0.0, 0.0, 0.0], ['comment', 'hi1vs5v', '2qixm', 'startrek', 'FALSE', '1635206397', 'https://old.reddit.com/r/startrek/comments/qftvn2/so_i_saw_dune_last_night_and_just_got_done/hi1vs5v/', '``', '*', 'The', 'first', 'duty', 'of', 'every', 'Starfleet', 'officer', 'is', 'to', 'the', 'truth', '.', 'Whether', 'it', "'s", 'scientific', 'truth', 'or', 'historical', 'truth', 'or', 'personal', 'truth', '.', 'It', 'is', 'the', 'guiding', 'principle', 'upon', 'which', 'Starfleet', 'is', 'based', '.', 'If', 'you', 'ca', "n't", 'find', 'it', 'within', 'yourself', 'to', 'stand', 'up', 'and', 'tell', 'the', 'truth', 'about', 'what', 'happened', 'you', 'do', "n't", 'deserve', 'to', 'wear', 'that', 'uniform', '.', '*', '[', 'Captain', 'Jean-Luc', 'Picard', '``', "''", 'The', 'First', 'Duty', "''", "''", ']', '(', 'https', ':', '//www.youtube.com/watch', '?', 'v=xefh7W1nVo4', ')', 'Reddit', 'admins', 'have', 'been', '[', 'ineffectual', 'in', 'their', 'response', 'to', 'COVID-19', 'misinformation', ']', '(', 'https', ':', '//www.dailydot.com/debug/subreddits-private-protest-covid-disinformation-reddit/', ')', '.', 'In', 'lieu', 'of', 'Reddit', 'gold', 'and', 'awards', 'we', 'ask', 'that', 'you', 'donate', 'to', 'the', '[', 'WHO', 'COVID-19', 'response', 'fund', ']', '(', 'https', ':', '//www.who.int/emergencies/diseases/novel-coronavirus-2019/donate', ')', '.', 'Please', 'respect', 'our', '[', 'subreddit', 'rules', ']', '(', 'https', ':', '//www.reddit.com/r/startrek/wiki/guidelines', ')', '.', 'LLAP', '!', '*', 'I', 'am', 'a', 'bot', 'and', 'this', 'action', 'was', 'performed', 'automatically', '.', 'Please', '[', 'contact', 'the', 'moderators', 'of', 'this', 'subreddit', ']', '(', '/message/compose/', '?', 'to=/r/startrek', ')', 'if', 'you', 'have', 'any', 'questions', 'or', 'concerns', '.', '*', "''", '0.9562', '1', 0.005555555555555556, 0.011111111111111112, 0.005555555555555556, 0.0, 0.005555555555555556, 0.011111111111111112, 0.044444444444444446, 0.0, 0.0, 0.03888888888888889], ['comment', 'abc', 'def', 'gh', 'FALSE', 'aaaa', 'bbbb', 'idiotic', '0', '0', 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0]]

'''


# Import necessary libraries
from pyspark import SparkContext
from nltk.tokenize import word_tokenize
import nltk
import pandas as pd

# Download the NLTK data
nltk.download('punkt')

# Create a Spark context
sc = SparkContext("local", "ReadCSV")

# Read the CSV file as an RDD of lines
lines = sc.textFile("your_input_file.csv")

# Skip the first line (header) of the CSV file
header = lines.first()
lines = lines.filter(lambda line: line != header)

# Define a function to assign group IDs based on 'comment' lines
def assign_group_id(line):
    if line.startswith("comment"):
        assign_group_id.group_id = line
    return (assign_group_id.group_id, line)

# Initialize the group_id attribute in the function
assign_group_id.group_id = None

# Assign group IDs to lines
grouped_lines = lines.map(assign_group_id)

# Group lines by the assigned group ID
grouped_lines = grouped_lines.groupByKey()

# Function to join all strings within an RDD and flatten the result
def flatten_and_join(iterable):
    flat_elements = []
    for element in iterable:
        flat_elements.extend(element.split(','))
    return flat_elements

# Apply the flatten_and_join function to each RDD to flatten and join the strings
flattened_rdd = grouped_lines.values().map(flatten_and_join)

# Tokenize the comment text within the RDD while keeping the rest of the elements
def tokenize_comment_text(line):
    comment_text = " ".join(line[7:-2])  # Join the comment text elements into a single string
    tokens = word_tokenize(comment_text)  # Tokenize the comment text
    line[7:-2] = tokens  # Replace the original comment text with tokens
    return line

# Tokenize the comment text in each element of the RDD while keeping the rest of the elements
tokenized_rdd = flattened_rdd.map(tokenize_comment_text)

# Create a dictionary where the keys are emotion names and the values are the corresponding emotion lexicon DataFrames
emotion_lexicons = {}

# Define a list of emotion names
emotions = ['anger', 'anticipation', 'disgust', 'fear', 'joy', 'negative', 'positive', 'sadness', 'surprise', 'trust']

# For each emotion, read the respective emotion lexicon file, rename columns, and filter for words where 'present' is 1
for emotion in emotions:
    lexicon = pd.read_csv(emotion + '-NRC-Emotion-Lexicon.txt', sep="\t", header=None)
    lexicon = lexicon.rename(columns={0: "word", 1: "present"})
    lexicon = lexicon[lexicon['present'] == 1]
    emotion_lexicons[emotion] = lexicon


# Helper function to match words
def contains_word(s, w):
    return (' ' + w + ' ') in (' ' + s + ' ')

# Function to calculate emotion scores for specific emotions from a predefined list
def calculate_emotion_scores(line, emotion_lexicon):
    text = " ".join(line[7:-2])  # Get the tokenized comment text
    if len(text) > 0:
        emotion_scores = {emotion: 0 for emotion in emotions}
        total_words = len(text.split())
        for emotion in emotions:
            lexicon = emotion_lexicons[emotion]
            for word in lexicon['word'].tolist():
                if contains_word(text, word):
                    emotion_scores[emotion] += 1
            emotion_scores[emotion] = emotion_scores[emotion] / total_words  # Normalize the counts
        for emotion in emotions:
            line.append(emotion_scores[emotion])  # Store the emotion score in line[10] onwards
    return line


# Apply the calculate_emotion_scores function to each element of the RDD for each emotion
for emotion, lexicon in emotion_lexicons.items():
    rdd_with_emotion_scores = tokenized_rdd.map(lambda x: calculate_emotion_scores(x, lexicon))

# Print the resulting RDD
print(rdd_with_emotion_scores.collect())

# Stop the Spark context
sc.stop()
